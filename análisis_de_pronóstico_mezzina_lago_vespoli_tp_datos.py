# -*- coding: utf-8 -*-
"""Análisis de pronóstico - Mezzina Lago Vespoli TP Datos

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y4NLwPoEfthiU6zIN1njxyrvfzcJeLDd
"""

# Paso 2: Importamos las librerías necesarias
import pandas as pd
from tqdm import tqdm

"""En esta parte cargamos la base de datos desde la computadora. Vamos a llamar a nuestra base de datos `df`.

"""

# Paso 1: Subir el archivo Excel desde tu computadora a Colab
from google.colab import files
uploaded = files.upload()  # Esto abrirá un diálogo para elegir el archivo

# Paso 2: Importamos pandas
import pandas as pd

# Paso 3: Cargamos el archivo Excel que acabamos de subir
# Cambia el nombre 'database_noticias.xlsx' por el nombre real del archivo que subas
df = pd.read_excel('database_noticias.xlsx', engine='openpyxl')

# Paso 4: Verificamos que se cargó correctamente
df.head()

"""Lo primero es modificar nuestra columna de "date" para que contenga solo el día de la noticia.
La hora a la que el bot la scrapeó no nos sirve.

"""

# Paso 4: Extraemos solo la fecha (sin hora)
# Convertimos la columna 'date' a datetime (por si no está)
df['date'] = pd.to_datetime(df['date'], errors='coerce')

# Creamos nueva columna con solo la fecha
df['date_only'] = df['date'].dt.date

# Eliminamos la columna original 'date'
df.drop(columns=['date'], inplace=True)

# Renombramos 'date_only' a 'date'
df.rename(columns={'date_only': 'date'}, inplace=True)

df.head()

"""Ahora vamos a crear una nueva columna que combine el título y el texto de cada noticia en una sola cadena.
Esto nos servirá para el procesamiento posterior.

"""

# Paso 5: Combinar título y texto en una sola columna 'full_text'
df['full_text'] = df['title'].fillna('') + ' ' + df['text'].fillna('')

# Verificamos
df[['full_text']].head()

"""Ahora, creamos una nueva base de datos llamada df_count que nos va a decir cuantas noticias de economía se publicaron en cada fecha. Es decir, simplemente dos columnas: una para la fecha, y otra ("count) nos dice cuantas noticias se publicaron ese día. Lo chequee manualmente y funciona bien."""

# Agrupamos el DataFrame original por la columna 'date' y contamos la cantidad de noticias por día
# Usamos groupby() con size() y luego reseteamos el índice para tener un DataFrame ordenado
df_count = df.groupby('date').size().reset_index(name='count')

# Ordenamos el DataFrame por fecha (opcional, para una visualización más clara)
df_count.sort_values('date', inplace=True)

# Mostramos las primeras filas para verificar el resultado
print(df_count.head())

"""El objetivo es transformar el DataFrame df para que tenga el mismo formato que noticias_para_etiquetar. Para ello:

Eliminar columnas irrelevantes: url, title, author, text.

Renombrar full_text a text (para consistencia con el dataset etiquetado).

Reordenar las columnas en este orden: id, text, date.

El DataFrame resultante será más limpio y listo para análisis y modelado.
"""

# Eliminamos las columnas irrelevantes del DataFrame original
df = df.drop(columns=['url', 'title', 'author', 'text'])

# Renombramos la columna 'full_text' a 'text'
df = df.rename(columns={'full_text': 'text'})

# Reordenamos las columnas en el orden deseado: id, text, date
df = df[['id', 'text', 'date']]

# Mostramos las primeras filas para verificar la estructura final
print(df.head())

"""Ahora, vamos a cargar a nuestro entorno el grupo de 1000 noticias clasificadas manualmente, y llamarlo df_label."""

# Paso 1: Subir el archivo Excel desde tu computadora a Colab
from google.colab import files
uploaded = files.upload()  # Esto abrirá un diálogo para elegir el archivo

# Paso 2: Cargamos el archivo Excel que subiste (usa el nombre exacto del archivo que aparece tras la subida)
df_labels = pd.read_excel('noticias_para_etiquetar.xlsx')

# Paso 3: Verificamos las primeras filas
print(df_labels.head())

"""Tiene algunas columnas unnamed que quedaron del excel, las eliminamos."""

# Eliminamos columnas que comienzan con 'Unnamed'
df_labels = df_labels.loc[:, ~df_labels.columns.str.contains('^Unnamed')]

# Verificamos la estructura después de limpiar
print(df_labels.head())
print(df_labels.columns)

"""**Tokenización**
Vamos a proceder a tokenizar los dataframes. Para la tokenización, utilizamos regex, que nos hace perder menos significado semántico que spacy. Específicamente, las reglas que sigue son:
✅ Qué conserva:

Letras minúsculas (a-z).

Vocales con tilde (áéíóú).

La letra ñ.

Números (0-9).

Símbolos económicos: $ y %.

✅ Qué elimina:

Puntos, comas, paréntesis, guiones.

Signos de interrogación y exclamación.

Caracteres especiales irrelevantes.

**Primero, tokenizamos el database de las noticias original, df**
"""

import re

# Función para tokenizar el texto
def tokenize_text(text):
    # Convertimos a minúsculas
    text = text.lower()
    # Aplicamos regex: extraemos palabras, números, $, %
    tokens = re.findall(r"[a-záéíóúñ0-9$%]+", text)
    return tokens

# Aplicamos la función a la columna 'text' del DataFrame df
df['tokens'] = df['text'].apply(tokenize_text)

# Mostramos las primeras 5 filas con todas las columnas (incluida 'tokens')
print(df.head(5))

# Función para tokenizar el texto (reutilizamos la misma)
def tokenize_text(text):
    # Convertimos a minúsculas
    text = text.lower()
    # Aplicamos regex: extraemos palabras, números, $, %
    tokens = re.findall(r"[a-záéíóúñ0-9$%]+", text)
    return tokens

# Aplicamos la función a la columna 'text' del DataFrame df_labels
df_labels['tokens'] = df_labels['text'].apply(tokenize_text)

# Mostramos las primeras 5 filas con todas las columnas (incluida 'tokens')
print(df_labels.head(5))

"""Entrenamos un diccionario con word2vec, que transforme palabras en vectores numéricos, capturando relaciones semánticas entre términos usados en las noticias.

Configuración del modelo (gensim Word2Vec):
sentences=df['tokens'] → las noticias tokenizadas.

vector_size=100 → tamaño del embedding (balance entre calidad y rapidez).

window=5 → contexto: considera 5 palabras a izquierda y derecha.

min_count=2 → ignora palabras que aparecen solo una vez (reduce ruido).

workers=4 → usa 4 núcleos para acelerar.

Antes, instalamos gensim en colab
"""

!pip install gensim

from gensim.models import Word2Vec

# 1. Preparamos la lista de listas de tokens (todas las noticias)
sentences = df['tokens'].tolist()

# 2. Entrenamos el modelo Word2Vec
model = Word2Vec(
    sentences=sentences,   # Nuestro corpus tokenizado
    vector_size=100,       # Dimensión del embedding
    window=5,              # Contexto (palabras cercanas)
    min_count=2,           # Ignora palabras que aparecen menos de 2 veces
    workers=4,             # Núcleos para paralelismo
    sg=1                   # Skip-gram (mejor para semántica)
)

# 3. Guardamos el modelo entrenado (opcional)
model.save("word2vec_noticias.model")

# 4. Información básica: tamaño del vocabulario
print(f"Tamaño del vocabulario aprendido: {len(model.wv)} palabras")

# 5. Ejemplo: vector de la palabra 'dólar' (si existe)
if 'dólar' in model.wv:
    print("Vector para 'dólar':", model.wv['dólar'][:10])  # Primeros 10 valores

# 6. Ejemplo: palabras más similares a 'dólar'
if 'dólar' in model.wv:
    print("Palabras más similares a 'dólar':", model.wv.most_similar('dólar', topn=5))

"""Ahora que el Word2Vec creo representaciones vectoriales con densidad semántica para cada palabra, podemos vectorizar todas las noticias para luego aplicar nuestro modelo de clasificación. Seguimos el procedimiento más común de simplemente promediar todos los vectores embedded dentro de una noticia para obtener 1 solo vector por noticia.
Tomamos la columna tokens de cada DataFrame (df_labels y df).

Para cada noticia:

Filtramos las palabras que estén en el vocabulario del modelo Word2Vec.

Obtenemos los vectores de cada palabra válida.

Calculamos el promedio de esos vectores para representar la noticia.

Si no hay ninguna palabra en el vocabulario, devolvemos un vector de ceros.

Resultado:

X_labels: matriz de forma (n_labels, 100).

X_all: matriz de forma (n_total, 100).
"""

import numpy as np

# Paso 1: Definimos la función para convertir una lista de tokens en un vector promedio
def get_average_vector(tokens, model, vector_size=100):
    """
    Convierte una lista de tokens en un vector promedio usando un modelo Word2Vec.
    - tokens: lista de palabras tokenizadas
    - model: modelo Word2Vec entrenado
    - vector_size: tamaño del embedding (por defecto 100)
    """
    valid_vectors = [model.wv[word] for word in tokens if word in model.wv]
    if valid_vectors:  # Si hay palabras válidas
        return np.mean(valid_vectors, axis=0)
    else:  # Si no hay palabras en el vocabulario, devolvemos un vector de ceros
        return np.zeros(vector_size)

# Paso 2: Generamos la matriz de vectores para df_labels
X_labels = np.array([get_average_vector(tokens, model) for tokens in df_labels['tokens']])

# Paso 3: Generamos la matriz de vectores para todo el corpus (df)
X_all = np.array([get_average_vector(tokens, model) for tokens in df['tokens']])

# Validamos dimensiones
print("Dimensión de X_labels:", X_labels.shape)
print("Dimensión de X_all:", X_all.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split

"""Limpiamos el database de noticias etiquetadas manualmente, porque hay algunos missing values."""

# Eliminamos filas donde 'label' es NaN
df_labels = df_labels.dropna(subset=['label'])

# Convertimos a int (por si hay floats)
df_labels['label'] = df_labels['label'].astype(int)

# Verificamos cuántas filas quedan
print("Filas después de limpiar:", len(df_labels))

# Extraemos de nuevo y_labels limpio
y_labels = df_labels['label'].values

# Ajustamos X_labels (mismo filtrado)
X_labels = np.array([get_average_vector(tokens, model) for tokens in df_labels['tokens']])

"""Ahora, el **modelo de regresión logística**.


"""

# Paso 1: Extraemos etiquetas
y_labels = df_labels['label'].values

# Paso 2: Dividimos en entrenamiento y prueba (80% / 20%)
X_train, X_test, y_train, y_test = train_test_split(X_labels, y_labels, test_size=0.2, random_state=42)

# Paso 3: Definimos el modelo de Regresión Logística
log_reg = LogisticRegression(max_iter=1000)

# Paso 4: Entrenamos el modelo
log_reg.fit(X_train, y_train)

# Paso 5: Predecimos sobre el conjunto de prueba
y_pred = log_reg.predict(X_test)

# Paso 6: Calculamos métricas
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, zero_division=0)
recall = recall_score(y_test, y_pred, zero_division=0)
f1 = f1_score(y_test, y_pred, zero_division=0)

print("Resultados del modelo en el conjunto de prueba:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

print(df_labels['label'].value_counts(normalize=True))

"""Estos resultados indican un problema grave en el modelo. Vamos a desglosarlo:

Accuracy = 0.93 (93%)

Parece excelente, pero es engañoso. Ocurre porque el dataset está muy desbalanceado (casi todas las noticias son clase 0). El modelo predice todo como clase 0 y aun así acierta el 93% (porque la mayoría son 0).

Precision = 0.0000

Entre las predicciones de clase 1 (si hubo), ninguna fue correcta.

Recall = 0.0000

El modelo no detectó ningún caso positivo real (label = 1). Esto significa que la clase minoritaria fue completamente ignorada. F1-score = 0.0000
Como F1 combina Precision y Recall, ambos son cero → F1 = 0.

Interpretación global
El modelo aprendió a predecir siempre la clase mayoritaria (0) porque:

Dataset desbalanceado (muchísimos 0, muy pocos 1).

Por defecto, la regresión logística minimiza errores globales → prefiere "jugar seguro" y clasificar todo como 0.

¿Por qué pasa esto?
Desbalance extremo en etiquetas (quizás 95% clase 0, 5% clase 1 o peor).

Sin ajuste por desbalance (class_weight en scikit-learn está en None por defecto).

Umbral estándar (0.5) → para una clase tan rara, casi nunca alcanza esa probabilidad.

Ajuste por desbalance con class_weight='balanced'
Objetivo:
Corregir el problema del desbalance (muchas clases 0 y pocas 1) asignando mayor peso a los errores de la clase minoritaria. Esto evita que el modelo prediga siempre 0.
Qué esperamos ver ahora:
Accuracy podría bajar (porque el modelo empezará a detectar clase 1).

Precision, Recall y F1-score de la clase 1 deben mejorar mucho.

La matriz de confusión mostrará más positivos detectados.

Qué esperamos ver ahora:
Accuracy podría bajar (porque el modelo empezará a detectar clase 1).

Precision, Recall y F1-score de la clase 1 deben mejorar mucho.

La matriz de confusión mostrará más positivos detectados.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt

# Paso 1: Confirmamos que no haya NaN en labels
df_labels = df_labels.dropna(subset=['label'])
df_labels['label'] = df_labels['label'].astype(int)

# Paso 2: Extraemos etiquetas
y_labels = df_labels['label'].values

# Paso 3: Dividimos en entrenamiento y prueba (80% / 20%)
X_train, X_test, y_train, y_test = train_test_split(X_labels, y_labels, test_size=0.2, random_state=42, stratify=y_labels)

# Paso 4: Definimos el modelo con ajuste por desbalance
log_reg = LogisticRegression(max_iter=1000, class_weight='balanced')

# Paso 5: Entrenamos el modelo
log_reg.fit(X_train, y_train)

# Paso 6: Predicciones
y_pred = log_reg.predict(X_test)

# Paso 7: Calculamos métricas
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, zero_division=0)
recall = recall_score(y_test, y_pred, zero_division=0)
f1 = f1_score(y_test, y_pred, zero_division=0)

print("Resultados con ajuste por desbalance:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

# Paso 8: Matriz de confusión
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Atraso', 'Atraso'], yticklabels=['No Atraso', 'Atraso'])
plt.title("Matriz de Confusión")
plt.ylabel("Etiqueta real")
plt.xlabel("Predicción")
plt.show()

"""Accuracy: 0.84
Bajó respecto al 0.93 anterior (esperado). Esto pasa porque ahora el modelo detecta muchos más casos positivos y comete más errores en la clase mayoritaria.

Precision: 0.2195 (21.95%)
De las predicciones que hizo como “CON atraso cambiario”, solo ~22% son correctas. Esto indica que el modelo sobre-predice la clase positiva (mete mucho ruido para no perder recall).

Recall: 1.0000 (100%)
¡Captura absolutamente todas las noticias que realmente son atraso cambiario!
✅ Muy bueno si tu objetivo es no dejar escapar ningún positivo (por ejemplo, alertas tempranas).

F1-score: 0.36
F1 subió de 0.0 a 0.36. Esto es enorme en comparación con antes, aunque no ideal.

✅ Interpretación estratégica
El modelo pasó de ignorar por completo la clase minoritaria a enfocarse en capturar todos los positivos, sacrificando precisión.

Es útil en escenarios donde es peor no detectar un caso positivo que detectar algunos falsos positivos (por ejemplo, monitoreo de riesgo cambiario).
"""

# Paso 1: Predecimos sobre todo el dataset
pred_all = log_reg.predict(X_all)

# Paso 2: Agregamos la columna al DataFrame original
df['pred_label'] = pred_all

# Paso 3: Verificamos las primeras filas
print(df.head())

# Paso 4: Distribución de clases predichas
print("\nDistribución de etiquetas predichas:")
print(df['pred_label'].value_counts())

# Paso 1: Contamos cuántas noticias predichas como 1 por fecha
df_ratio = df.groupby('date')['pred_label'].sum().reset_index()
df_ratio.columns = ['date', 'positivas']

# Paso 2: Combinamos con df_count (que tiene date y count)
df_count = df_count.merge(df_ratio, on='date', how='left')

# Paso 3: Calculamos el ratio
df_count['ratio'] = df_count['positivas'] / df_count['count']

# Paso 4: Verificamos las primeras filas
print(df_count.head())

# Estadísticas descriptivas para la columna 'ratio'
stats_ratio = df_count['ratio'].describe()

print("Estadísticas descriptivas para la columna 'ratio':")
print(stats_ratio)

import matplotlib.pyplot as plt

# Paso 1: Ordenamos por fecha (por si acaso)
df_count = df_count.sort_values('date')

# Paso 2: Graficamos la serie temporal
plt.figure(figsize=(12, 6))
plt.plot(df_count['date'], df_count['ratio'], marker='', linestyle='-', linewidth=1)

# Paso 3: Ajustamos etiquetas y título
plt.title('Proporción diaria de noticias sobre atraso cambiario', fontsize=14)
plt.xlabel('Fecha', fontsize=12)
plt.ylabel('Ratio (proporción)', fontsize=12)
plt.xticks(rotation=45)
plt.grid(alpha=0.3)

# Paso 4: Mostramos la gráfica
plt.show()

# --- Paso 1: Asegurar que la columna 'date' sea tipo datetime64 ---
df_count['date'] = pd.to_datetime(df_count['date'], errors='coerce')

# --- Paso 2: Filtrar filas donde ratio = 0 entre 2017 y 2018 ---
mask_gap = (
    (df_count['date'] >= pd.Timestamp('2017-01-01')) &
    (df_count['date'] <= pd.Timestamp('2018-12-31')) &
    (df_count['ratio'] == 0)
)
gap_data = df_count.loc[mask_gap]

# --- Paso 3: Analizar el hueco ---
print("Cantidad de días con ratio = 0 en 2017-2018:", len(gap_data))
print("\nResumen del hueco:")
print(gap_data.describe())

# ¿Hay noticias en esas fechas?
total_news = gap_data['count'].sum()
unique_counts = gap_data['count'].unique()
print(f"\nTotal de noticias en esas fechas: {total_news}")
print(f"Valores únicos de 'count': {unique_counts}")

# --- Paso 4: Visualizar el hueco en la serie ---
plt.figure(figsize=(12, 6))
plt.plot(df_count['date'], df_count['ratio'], label='Ratio diario', alpha=0.7)
plt.scatter(gap_data['date'], gap_data['ratio'], color='red', label='Hueco (ratio=0)', s=10)
plt.title('Proporción diaria de noticias sobre atraso cambiario (con hueco resaltado)')
plt.xlabel('Fecha')
plt.ylabel('Ratio')
plt.legend()
plt.show()

"""2017 Capturo solo 67 días de noticias. Esto va a empantanar todo el trabajo con series temporales. Lo conveniente es eliminar directamente los días de 2017 sin noticias. Veamos nuestro dataframe."""

# --- Paso 1: Determinar los límites del hueco ---
fecha_inicio_gap = gap_data['date'].min()
fecha_fin_gap = gap_data['date'].max()

print(f"Hueco desde {fecha_inicio_gap.date()} hasta {fecha_fin_gap.date()}")

# --- Paso 2: Crear los dos DataFrames ---
df_before = df_count[df_count['date'] < fecha_inicio_gap][['date', 'ratio']].reset_index(drop=True)
df_after = df_count[df_count['date'] > fecha_fin_gap][['date', 'ratio']].reset_index(drop=True)

# --- Verificación ---
print("\nPrimeras filas de df_before:")
print(df_before.head())
print("\nÚltimas filas de df_before:")
print(df_before.tail())

print("\nPrimeras filas de df_after:")
print(df_after.head())
print("\nÚltimas filas de df_after:")
print(df_after.tail())

import matplotlib.pyplot as plt

# --- Gráfico 1: Serie antes del hueco ---
plt.figure(figsize=(12, 5))
plt.plot(df_before['date'], df_before['ratio'], label='Cociente', alpha=0.8)
plt.title('Proporción de noticias que hablan sobre atraso cambiario (hasta 2016-12-31)')
plt.xlabel('Fecha')
plt.ylabel('Ratio')
plt.grid(alpha=0.3)
plt.legend()
plt.show()

# --- Gráfico 2: Serie después del hueco ---
plt.figure(figsize=(12, 5))
plt.plot(df_after['date'], df_after['ratio'], label='Cociente', alpha=0.8, color='orange')
plt.title('Proporción de noticias que hablan sobre atraso cambiario (desde 2018-12-28)')
plt.xlabel('Fecha')
plt.ylabel('Ratio')
plt.grid(alpha=0.3)
plt.legend()
plt.show()

"""Ahora, para la serie de dólar del BCRA."""

# ===========================
# Descarga y filtrado de la serie “usd_of” (tipo de cambio oficial BCRA)
# ===========================
# Requisitos: pip install requests pandas

import requests                 # Para hacer el GET
import pandas as pd             # Para manipular la serie

# 1) Token de autenticación (proporcionado por el BCRA)
TOKEN = (
    "eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9."
    "eyJleHAiOjE3ODUyMTI4MTUsInR5cGUiOiJleHRlcm5hbCIsInVzZXIiOiJzYW50aWFn"
    "b2VybmVzdG8ubGFnb0BnbWFpbC5jb20ifQ."
    "dm4RmFSUPKZAX5nhcgwIhPCcat0zJLE8ix_QR6h4fAjArHmXNMXrmC_1BRB9Nxbv0v6K"
    "pC-LWbBTj0LOCLsitQ"
)

# 2) Endpoint de la serie mensual “usd_of”
URL = "https://api.estadisticasbcra.com/usd_of"

# 3) Realizamos la solicitud GET con header de autorización
headers = {"Authorization": f"BEARER {TOKEN}"}
response = requests.get(URL, headers=headers, timeout=10)
response.raise_for_status()     # Lanza error si el request no es 200 OK

# 4) Convertimos la respuesta JSON en DataFrame
data = response.json()
usd_of = pd.DataFrame(data)               # Columnas originales: ['d', 'v']
usd_of.columns = ["date", "usd_of"]       # Renombramos
usd_of["date"] = pd.to_datetime(usd_of["date"], errors="coerce")
usd_of["usd_of"] = pd.to_numeric(usd_of["usd_of"], errors="coerce")

# 5) Filtramos al período de interés (2011‑01‑03 ↔ 2025‑07‑15)
start_date = pd.Timestamp("2011-01-03")
end_date   = pd.Timestamp("2025-07-15")
usd_of = usd_of.loc[
    (usd_of["date"].between(start_date, end_date))
].sort_values("date").reset_index(drop=True)

# 6) Vista rápida de la serie resultante
print("Primeras filas:\n", usd_of.head(), "\n")
print("Últimas filas:\n", usd_of.tail())

# 1) Aseguramos orden y convertimos 'date' a índice
usd_of = usd_of.sort_values('date')
usd_of = usd_of.set_index('date')

# 2) Graficamos la serie
plt.figure(figsize=(12, 5))
plt.plot(usd_of.index, usd_of['usd_of'])
plt.title('Tipo de cambio oficial (ARS/USD) – BCRA')
plt.xlabel('Fecha')
plt.ylabel('ARS por USD')
plt.grid(True)
plt.tight_layout()
plt.show()

"""Para homogenizar las series, vamos a eliminar las observaciones con fecha igual a las que eliminamos de la serie anterior."""

print(usd_of.columns)

# --- Paso 1: Pasar el índice (fechas) a columna ---
usd_of = usd_of.reset_index()

# --- Paso 2: Renombrar la columna de fechas a 'date' ---
# El índice se llamó probablemente 'index', lo renombramos
usd_of.rename(columns={'index': 'date'}, inplace=True)

# --- Paso 3: Convertir 'date' a datetime ---
usd_of['date'] = pd.to_datetime(usd_of['date'], errors='coerce')

# --- Paso 4: Definir los límites del corte ---
inicio_before = pd.Timestamp('2011-01-03')
fin_before = pd.Timestamp('2016-12-31')

inicio_after = pd.Timestamp('2018-12-28')
fin_after = pd.Timestamp('2025-07-15')

# --- Paso 5: Crear usd1 y usd2 ---
usd1 = usd_of[(usd_of['date'] >= inicio_before) & (usd_of['date'] <= fin_before)].reset_index(drop=True)
usd2 = usd_of[(usd_of['date'] >= inicio_after) & (usd_of['date'] <= fin_after)].reset_index(drop=True)

# --- Verificación ---
print("usd1:", usd1.shape)
print(usd1.head(), "\n")
print("usd2:", usd2.shape)
print(usd2.head())

import matplotlib.pyplot as plt

# --- Gráfico 1: USD oficial antes del hueco ---
plt.figure(figsize=(12, 5))
plt.plot(usd1['date'], usd1['usd_of'], label='USD oficial', alpha=0.8)
plt.title('Tipo de cambio oficial antes del hueco (2011-01-03 a 2016-12-31)')
plt.xlabel('Fecha')
plt.ylabel('USD oficial (ARS)')
plt.grid(alpha=0.3)
plt.legend()
plt.show()

# --- Gráfico 2: USD oficial después del hueco ---
plt.figure(figsize=(12, 5))
plt.plot(usd2['date'], usd2['usd_of'], label='USD oficia', alpha=0.8, color='orange')
plt.title('Tipo de cambio oficial después del hueco (2018-12-28 a 2025-07-15)')
plt.xlabel('Fecha')
plt.ylabel('USD oficial (ARS)')
plt.grid(alpha=0.3)
plt.legend()
plt.show()

print(df_before.head())
print(df_after.head())
print(usd1.head())
print(usd2.head())

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# --- Paso 1: Unir series por fecha ---
before = pd.merge(df_before, usd1, on='date', how='inner')
after = pd.merge(df_after, usd2, on='date', how='inner')

# --- Paso 2: Calcular diferencias ---
before['d_ratio'] = before['ratio'].diff()
before['d_log_usd'] = np.log(before['usd_of']).diff()

after['d_ratio'] = after['ratio'].diff()
after['d_log_usd'] = np.log(after['usd_of']).diff()

# Eliminamos el primer valor NaN por la diferencia
before = before.dropna(subset=['d_ratio', 'd_log_usd'])
after = after.dropna(subset=['d_ratio', 'd_log_usd'])

# --- Gráfico: Período antes del hueco ---
plt.figure(figsize=(14, 6))
plt.plot(before['date'], before['d_ratio'], label='Δ ratio', alpha=0.7)
plt.plot(before['date'], before['d_log_usd'], label='Δ log(USD)', alpha=0.7)
plt.title('Evolución de menciones a atraso cambiario (en diferencias de ratios) y del tipo de cambio (en diferencias de logaritmos) (2011-2016)')
plt.xlabel('Fecha')
plt.ylabel('Cambio')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# --- Gráfico: Período después del hueco ---
plt.figure(figsize=(14, 6))
plt.plot(after['date'], after['d_ratio'], label='Δ ratio', alpha=0.7)
plt.plot(after['date'], after['d_log_usd'], label='Δ log(USD)', alpha=0.7)
plt.title('Evolución de menciones a atraso cambiario (en diferencias de ratios) y del tipo de cambio (en diferencias de logaritmos) (2018-2025)')
plt.xlabel('Fecha')
plt.ylabel('Cambio')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

"""CHEQUEAMIS ESTACIONARIEDAD"""

from statsmodels.tsa.stattools import adfuller

def adf_test(series, name):
    result = adfuller(series, autolag='AIC')
    print(f"--- ADF Test for {name} ---")
    print(f"ADF Statistic: {result[0]:.4f}")
    print(f"p-value: {result[1]:.4f}")
    print(f"Critical Values: {result[4]}")
    if result[1] < 0.05:
        print("=> Serie estacionaria (rechazamos H0)\n")
    else:
        print("=> Serie NO estacionaria (no rechazamos H0)\n")

# --- Antes del hueco ---
adf_test(before['d_ratio'], 'd_ratio (antes)')
adf_test(before['d_log_usd'], 'd_log_usd (antes)')

# --- Después del hueco ---
adf_test(after['d_ratio'], 'd_ratio (después)')
adf_test(after['d_log_usd'], 'd_log_usd (después)')

from statsmodels.tsa.stattools import ccf
import matplotlib.pyplot as plt
import numpy as np

def plot_ccf(series_x, series_y, max_lag, title):
    # Calcular correlación cruzada
    corr = ccf(series_x, series_y)[:max_lag+1]
    lags = np.arange(0, max_lag+1)

    plt.figure(figsize=(12, 6))
    plt.bar(lags, corr, width=0.4)
    plt.axhline(0, color='black', linewidth=1)
    plt.title(f'Correlación cruzada (hasta {max_lag} rezagos) - {title}')
    plt.xlabel('Rezago')
    plt.ylabel('Correlación')
    plt.show()

# --- Antes del hueco ---
plot_ccf(before['d_ratio'], before['d_log_usd'], max_lag=15, title='Antes del hueco (ratio → USD)')

# --- Después del hueco ---
plot_ccf(after['d_ratio'], after['d_log_usd'], max_lag=15, title='Después del hueco (ratio → USD)')

from statsmodels.tsa.api import VAR
from statsmodels.tsa.stattools import grangercausalitytests

# --- Preparación de datos para VAR (Antes del hueco) ---
before_var = before[['d_log_usd', 'd_ratio']]
after_var = after[['d_log_usd', 'd_ratio']]

# --- Paso 1: Selección de rezagos óptimos ---
model_before = VAR(before_var)
lag_before = model_before.select_order(maxlags=15)
print("\nCriterios de selección (Antes del hueco):")
print(lag_before.summary())

model_after = VAR(after_var)
lag_after = model_after.select_order(maxlags=15)
print("\nCriterios de selección (Después del hueco):")
print(lag_after.summary())

# Tomamos el lag óptimo según AIC
opt_lag_before = lag_before.aic
opt_lag_after = lag_after.aic

# --- Paso 2: Test de Granger ---
print("\n--- Granger antes del hueco ---")
granger_before = grangercausalitytests(before_var, maxlag=opt_lag_before, verbose=True)

print("\n--- Granger después del hueco ---")
granger_after = grangercausalitytests(after_var, maxlag=opt_lag_after, verbose=True)

"""Ahora vamos con el dataset clasificado por Bert"""

from google.colab import files
import pandas as pd

# Paso 1: Subimos el archivo desde tu computadora
uploaded = files.upload()  # Esto abrirá el selector de archivos

# Paso 2: Leemos el archivo
df_extra = pd.read_excel('dataset_con_predicciones.xlsx')

# Paso 3: Mostramos las primeras filas
print("Primeras filas del dataset:")
print(df_extra.head())

# Paso 4: Información general del dataset
print("\nInformación general:")
print(df_extra.info())

# Paso 5: Conteo por etiqueta (si existe columna 'label')
if 'label' in df_extra.columns:
    print("\nDistribución de etiquetas:")
    print(df_extra['label'].value_counts())

# Paso 1: Conservamos las columnas relevantes
df_extra_clean = df_extra[['id', 'date', 'text_clean', 'clasificacion']].copy()

# Paso 2: Convertimos la columna date a solo fecha (sin hora)
df_extra_clean['date'] = pd.to_datetime(df_extra_clean['date']).dt.date

# Paso 3: Convertimos clasificacion a números (0 y 1)
df_extra_clean['clasificacion'] = df_extra_clean['clasificacion'].replace({
    'SIN atraso cambiario': 0,
    'CON atraso cambiario': 1
}).astype(int)

# Paso 4: Renombramos la columna a 'label'
df_extra_clean.rename(columns={'clasificacion': 'label'}, inplace=True)

# Paso 5: Verificamos las primeras filas
print(df_extra_clean.head())

# Paso 6: Chequeamos la distribución de etiquetas
print("\nDistribución de etiquetas en df_extra_clean:")
print(df_extra_clean['label'].value_counts())

import pandas as pd
import matplotlib.pyplot as plt

# --- Paso 1: Convertir fecha ---
df_extra_clean['date'] = pd.to_datetime(df_extra_clean['date'], errors='coerce')

# --- Paso 2: Agrupar por fecha ---
df_ratio_clean = df_extra_clean.groupby('date').agg(
    count=('label', 'size'),
    positivas=('label', 'sum')
).reset_index()

# --- Paso 3: Calcular ratio ---
df_ratio_clean['ratio'] = df_ratio_clean['positivas'] / df_ratio_clean['count']

# --- Verificación ---
print(df_ratio_clean.head())

# --- Paso 4: Graficar ---
plt.figure(figsize=(14, 6))
plt.plot(df_ratio_clean['date'], df_ratio_clean['ratio'], alpha=0.7)
plt.title('Proporción diaria de noticias sobre atraso cambiario (Etiquetas manuales)')
plt.xlabel('Fecha')
plt.ylabel('Ratio')
plt.grid(alpha=0.3)
plt.show()

import matplotlib.pyplot as plt
import pandas as pd

# --- Fechas exactas ---
inicio_serie1 = pd.Timestamp('2011-01-03')
fin_serie1 = pd.Timestamp('2016-12-31')

inicio_serie2 = pd.Timestamp('2018-12-28')
fin_serie2 = pd.Timestamp('2025-07-15')

# --- Crear las dos series ---
ratio_antes_2017 = df_ratio_clean[(df_ratio_clean['date'] >= inicio_serie1) &
                                  (df_ratio_clean['date'] <= fin_serie1)].reset_index(drop=True)

ratio_despues_2018 = df_ratio_clean[(df_ratio_clean['date'] >= inicio_serie2) &
                                    (df_ratio_clean['date'] <= fin_serie2)].reset_index(drop=True)

# --- Verificación ---
print(f"Serie antes de 2017: {ratio_antes_2017.shape}")
print(ratio_antes_2017.head(), "\n")
print(f"Serie después de 2018-12-28: {ratio_despues_2018.shape}")
print(ratio_despues_2018.head())

# --- Graficar ---
# Gráfico 1
plt.figure(figsize=(14, 6))
plt.plot(ratio_antes_2017['date'], ratio_antes_2017['ratio'],
         label='Ratio antes del 2017', alpha=0.7)
plt.title('Proporción diaria (hasta 2016-12-31) - Etiquetado manual')
plt.xlabel('Fecha')
plt.ylabel('Ratio')
plt.grid(alpha=0.3)
plt.legend()
plt.show()

# Gráfico 2
plt.figure(figsize=(14, 6))
plt.plot(ratio_despues_2018['date'], ratio_despues_2018['ratio'],
         color='orange', label='Ratio después del 2018-12-28', alpha=0.7)
plt.title('Proporción diaria (desde 2018-12-28) - Etiquetado manual')
plt.xlabel('Fecha')
plt.ylabel('Ratio')
plt.grid(alpha=0.3)
plt.legend()
plt.show()

import pandas as pd
import numpy as np
from statsmodels.tsa.api import VAR
from statsmodels.tsa.stattools import grangercausalitytests, adfuller

# --- Paso 1: Unir series ---
before = pd.merge(ratio_antes_2017[['date','ratio']], usd1[['date','usd_of']], on='date', how='inner')
after = pd.merge(ratio_despues_2018[['date','ratio']], usd2[['date','usd_of']], on='date', how='inner')

# --- Paso 2: Calcular diferencias ---
before['d_ratio'] = before['ratio'].diff()
before['d_log_usd'] = np.log(before['usd_of']).diff()

after['d_ratio'] = after['ratio'].diff()
after['d_log_usd'] = np.log(after['usd_of']).diff()

# --- Paso 3: Eliminar NaN ---
before = before.dropna()
after = after.dropna()

# --- Paso 4: Test ADF ---
def adf_check(series, name):
    stat, pval, _, _, crit, _ = adfuller(series, autolag='AIC')
    print(f"{name}: ADF={stat:.3f}, p-value={pval:.4f}")

print("=== Test ADF ===")
adf_check(before['d_ratio'], 'd_ratio (antes)')
adf_check(before['d_log_usd'], 'd_log_usd (antes)')
adf_check(after['d_ratio'], 'd_ratio (después)')
adf_check(after['d_log_usd'], 'd_log_usd (después)')

# Selección de rezagos corregida
opt_lag_before = lag_before.aic
opt_lag_after = lag_after.aic

print(f"Lag óptimo (antes): {opt_lag_before}")
print(f"Lag óptimo (después): {opt_lag_after}")

# Test de Granger
print("\n=== Granger antes del 2017 ===")
granger_before = grangercausalitytests(before[['d_log_usd','d_ratio']], maxlag=opt_lag_before, verbose=True)

print("\n=== Granger después del 2018-12-28 ===")
granger_after = grangercausalitytests(after[['d_log_usd','d_ratio']], maxlag=opt_lag_after, verbose=True)

import matplotlib.pyplot as plt
import numpy as np

# --- Calcular diferencias ---
# Para el primer período
before['d_ratio'] = before['ratio'].diff()
before['d_log_usd'] = np.log(before['usd_of']).diff()

# Para el segundo período
after['d_ratio'] = after['ratio'].diff()
after['d_log_usd'] = np.log(after['usd_of']).diff()

# Eliminar NaN
before = before.dropna()
after = after.dropna()

# --- Gráfico 1: Antes del 2017 ---
plt.figure(figsize=(14, 6))
plt.plot(before['date'], before['d_ratio'], label='Δ Ratio (diario)', alpha=0.7)
plt.plot(before['date'], before['d_log_usd'], label='Δ log(USD)', alpha=0.7)
plt.axhline(0, color='black', linewidth=0.8, linestyle='--')
plt.title('Variaciones diarias: Ratio vs Tipo de Cambio (hasta 2016-12-31)')
plt.xlabel('Fecha')
plt.ylabel('Cambio')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# --- Gráfico 2: Después del 2018-12-28 ---
plt.figure(figsize=(14, 6))
plt.plot(after['date'], after['d_ratio'], label='Δ Ratio (diario)', alpha=0.7)
plt.plot(after['date'], after['d_log_usd'], label='Δ log(USD)', alpha=0.7)
plt.axhline(0, color='black', linewidth=0.8, linestyle='--')
plt.title('Variaciones diarias: Ratio vs Tipo de Cambio (desde 2018-12-28)')
plt.xlabel('Fecha')
plt.ylabel('Cambio')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# --- Filtrar hasta 2018 ---
df_semanal = df_ratio_clean[df_ratio_clean['date'] <= '2018-12-27'].copy()

# Paso 1: Reamostrar ratio semanal
weekly_ratio = df_semanal.resample('W', on='date').agg({
    'count': 'sum',
    'positivas': 'sum'
})
weekly_ratio['ratio'] = weekly_ratio['positivas'] / weekly_ratio['count']

# Paso 2: Reamostrar USD oficial (usd1)
usd1_weekly = usd1.resample('W', on='date').agg({
    'usd_of': 'mean'
})

# Paso 3: Combinar
weekly_data = weekly_ratio.join(usd1_weekly, how='inner').dropna()

# --- Graficar ---
import matplotlib.pyplot as plt

fig, ax1 = plt.subplots(figsize=(14, 6))

# Ratio (escala izquierda)
ax1.plot(weekly_data.index, weekly_data['ratio'], color='blue', label='Ratio semanal')
ax1.set_ylabel('Ratio menciones', color='blue')
ax1.tick_params(axis='y', labelcolor='blue')

# USD (escala derecha)
ax2 = ax1.twinx()
ax2.plot(weekly_data.index, weekly_data['usd_of'], color='orange', label='USD oficial (promedio semanal)')
ax2.set_ylabel('USD oficial', color='orange')
ax2.tick_params(axis='y', labelcolor='orange')

plt.title('Ratio semanal vs Tipo de Cambio Oficial (hasta 2018)')
fig.tight_layout()
plt.grid(alpha=0.3)
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# --- Calcular variaciones ---
weekly_data['d_ratio'] = weekly_data['ratio'].diff()
weekly_data['d_log_usd'] = np.log(weekly_data['usd_of']).diff()

# Eliminar NaN
weekly_diff = weekly_data.dropna(subset=['d_ratio', 'd_log_usd'])

# --- Graficar variaciones ---
plt.figure(figsize=(14, 6))
plt.plot(weekly_diff.index, weekly_diff['d_ratio'], label='Δ Ratio semanal', alpha=0.7)
plt.plot(weekly_diff.index, weekly_diff['d_log_usd'], label='Δ log(USD semanal)', alpha=0.7)
plt.axhline(0, color='black', linewidth=0.8, linestyle='--')
plt.title('Variaciones semanales: Ratio vs Tipo de Cambio Oficial (hasta 2018)')
plt.xlabel('Fecha')
plt.ylabel('Cambio')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

from statsmodels.tsa.api import VAR
from statsmodels.tsa.stattools import grangercausalitytests

# --- Preparar datos ---
weekly_diff_granger = weekly_diff[['d_log_usd', 'd_ratio']]

# --- Selección de rezago óptimo ---
model_weekly = VAR(weekly_diff_granger)
lag_selection = model_weekly.select_order(maxlags=8)
opt_lag_weekly = lag_selection.aic  # Lag óptimo según AIC
print(f"Lag óptimo (semanal, hasta 2018): {opt_lag_weekly}")

# --- Test de Granger ---
granger_weekly = grangercausalitytests(weekly_diff_granger, maxlag=opt_lag_weekly, verbose=True)

# --- Filtrar hasta 2018 ---
df_mensual = df_ratio_clean[df_ratio_clean['date'] <= '2018-12-27'].copy()

# Paso 1: Reamostrar ratio mensual
monthly_ratio = df_mensual.resample('M', on='date').agg({
    'count': 'sum',
    'positivas': 'sum'
})
monthly_ratio['ratio'] = monthly_ratio['positivas'] / monthly_ratio['count']

# Paso 2: Reamostrar USD oficial (usd1)
usd1_monthly = usd1.resample('M', on='date').agg({
    'usd_of': 'mean'
})

# Paso 3: Combinar
monthly_data = monthly_ratio.join(usd1_monthly, how='inner').dropna()

# Paso 4: Calcular diferencias
monthly_data['d_ratio'] = monthly_data['ratio'].diff()
monthly_data['d_log_usd'] = np.log(monthly_data['usd_of']).diff()
monthly_diff = monthly_data.dropna()

# --- Gráfico para ver la relación ---
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
plt.plot(monthly_diff.index, monthly_diff['d_ratio'], label='Δ Ratio mensual', alpha=0.7)
plt.plot(monthly_diff.index, monthly_diff['d_log_usd'], label='Δ log(USD mensual)', alpha=0.7)
plt.axhline(0, color='black', linewidth=0.8, linestyle='--')
plt.title('Variaciones mensuales: Ratio vs Tipo de Cambio Oficial (hasta 2018)')
plt.xlabel('Fecha')
plt.ylabel('Cambio')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Paso 5: Test de Granger
from statsmodels.tsa.api import VAR
from statsmodels.tsa.stattools import grangercausalitytests

model_monthly = VAR(monthly_diff[['d_log_usd', 'd_ratio']])
lag_selection_monthly = model_monthly.select_order(maxlags=6)
opt_lag_monthly = lag_selection_monthly.aic
print(f"Lag óptimo (mensual, hasta 2018): {opt_lag_monthly}")

granger_monthly = grangercausalitytests(monthly_diff[['d_log_usd', 'd_ratio']], maxlag=opt_lag_monthly, verbose=True)